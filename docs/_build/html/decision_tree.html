<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>&lt;no title&gt; &mdash; dectree 1.0 documentation</title>
    
    <link rel="stylesheet" href="_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '1.0',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <link rel="top" title="dectree 1.0 documentation" href="index.html" />
    <link rel="prev" title="&lt;no title&gt;" href="criteria.html" />
   
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head>
  <body role="document">  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <span class="target" id="module-decision_tree"></span><p>Module containing the DecisionTree, TreeNode and NodeSplit classes.</p>
<dl class="class">
<dt id="decision_tree.ContingencyTable">
<em class="property">class </em><code class="descclassname">decision_tree.</code><code class="descname">ContingencyTable</code><span class="sig-paren">(</span><em>contingency_table</em>, <em>values_num_samples</em><span class="sig-paren">)</span><a class="headerlink" href="#decision_tree.ContingencyTable" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">tuple</span></code></p>
<p>Contains the information about an attribute&#8217;s contingency table. When empty, defaults to
<cite>(None, None)</cite>. Otherwise, contains a np.array in each entry.</p>
<dl class="attribute">
<dt id="decision_tree.ContingencyTable.contingency_table">
<code class="descname">contingency_table</code><a class="headerlink" href="#decision_tree.ContingencyTable.contingency_table" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 0</p>
</dd></dl>

<dl class="method">
<dt id="decision_tree.ContingencyTable.count">
<code class="descname">count</code><span class="sig-paren">(</span><em>value</em><span class="sig-paren">)</span> &rarr; integer -- return number of occurrences of value<a class="headerlink" href="#decision_tree.ContingencyTable.count" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="decision_tree.ContingencyTable.index">
<code class="descname">index</code><span class="sig-paren">(</span><em>value</em><span class="optional">[</span>, <em>start</em><span class="optional">[</span>, <em>stop</em><span class="optional">]</span><span class="optional">]</span><span class="sig-paren">)</span> &rarr; integer -- return first index of value.<a class="headerlink" href="#decision_tree.ContingencyTable.index" title="Permalink to this definition">¶</a></dt>
<dd><p>Raises ValueError if the value is not present.</p>
</dd></dl>

<dl class="attribute">
<dt id="decision_tree.ContingencyTable.values_num_samples">
<code class="descname">values_num_samples</code><a class="headerlink" href="#decision_tree.ContingencyTable.values_num_samples" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 1</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="decision_tree.DecisionTree">
<em class="property">class </em><code class="descclassname">decision_tree.</code><code class="descname">DecisionTree</code><span class="sig-paren">(</span><em>criterion</em><span class="sig-paren">)</span><a class="headerlink" href="#decision_tree.DecisionTree" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">object</span></code></p>
<p>Data structure containing basic information pertaining to the whole tree.</p>
<p>This class&#8217; state should be accessed only indirectly, through its methods.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>criterion</strong> (<a class="reference internal" href="criteria.html#criteria.Criterion" title="criteria.Criterion"><em>criteria.Criterion</em></a>) &#8211; criterion which will be used to generate the tree
nodes/splits.</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="decision_tree.DecisionTree.cross_validate">
<code class="descname">cross_validate</code><span class="sig-paren">(</span><em>curr_dataset</em>, <em>num_folds</em>, <em>max_depth</em>, <em>min_samples_per_node</em>, <em>is_stratified=True</em>, <em>print_tree=False</em>, <em>seed=None</em>, <em>print_samples=False</em>, <em>use_stop_conditions=False</em>, <em>max_p_value_chi_sq=0.1</em><span class="sig-paren">)</span><a class="headerlink" href="#decision_tree.DecisionTree.cross_validate" title="Permalink to this definition">¶</a></dt>
<dd><p>Does a cross-validation using a given dataset.</p>
<p>It splits this dataset in <cite>num_folds</cite> folds and calls <cite>train_and_test</cite> on each. Might
be given a seed for the dataset&#8217;s random splitting and might be stratified.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>curr_dataset</strong> (<a class="reference internal" href="dataset.html#dataset.Dataset" title="dataset.Dataset"><em>dataset.Dataset</em></a>) &#8211; dataset containing the samples used for training.</li>
<li><strong>num_folds</strong> (<em>int</em>) &#8211; number of folds used in the cross-validation.</li>
<li><strong>max_depth</strong> (<em>int</em>) &#8211; maximum tree depth allowed. Zero means the root is a leaf.</li>
<li><strong>min_samples_per_node</strong> (<em>int</em>) &#8211; if a node has less than this number of training samples,
it will necessarily be a leaf.</li>
<li><strong>is_stratified</strong> (<em>bool</em>) &#8211; Indicates wheter the cross-validation should be stratified or
just a simple k-fold cross-validation. Stratified means the samples&#8217; splitting will try
to keep the classes&#8217; distribution the same across folds. Defaults to <cite>True</cite>.</li>
<li><strong>print_tree</strong> (<em>bool</em>) &#8211; Indicates wether the trees of every fold should be printed to
stdout. Defaults to <cite>False</cite>.</li>
<li><strong>seed</strong> (<em>int or None</em>) &#8211; indicates the seed that should be used to generate the random samples&#8217;
splitting in folds. If <cite>None</cite>, a random seed is used. Defaults to <cite>None</cite>.</li>
<li><strong>print_samples</strong> (<em>bool</em>) &#8211; if <cite>True</cite>, prints the samples indices used at each fold. Used for
debugging. Defaults to <cite>False</cite>.</li>
<li><strong>use_stop_conditions</strong> (<em>bool</em>) &#8211; informs wether we should use pruning techniques to avoid
using attributes with small number of samples (and, thus, avoiding statistical
anomalies). An attribute will be considered invalid if it contains less than
<cite>MIN_SAMPLES_IN_SECOND_MOST_FREQUENT_VALUE</cite> samples in the second most frequent value
(this way at least two values have this number of samples) or if a chi-square test,
applied on the attributes&#8217; contingency table has a p-value greater or equal to
<cite>max_p_value_chi_sq</cite>. When an attribute is considered invalid for the number of samples
in the second most frequent value, it will be considered invalid in every child node of
the current TreeNode. If it was considered invalid because of the chi-square test, it
can be considered valid in a descendant node. Note that numeric attributes are never
tested in this way. Defaults to <cite>False</cite>.</li>
<li><strong>max_p_value_chi_sq</strong> (<em>float</em>) &#8211; is the maximum p-value allowed for an attribute to be
accepted when doing chi-square tests (that is, when <cite>use_stop_conditions</cite> is <cite>True</cite>). A
p-value of 1.0 is equal to 100%. Defaults to <cite>0.1</cite> (which is 10%).</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><p>A tuple containing, in order:</p>
<ul class="simple">
<li>a list of predicted class for each sample;</li>
<li>the number of correct classifications;</li>
<li>the number of correct classifications done without samples with unkown values; (that
is, values that are unkown at a TreeNode &#8211; they are classified as the most common
class at that node);</li>
<li>the total cost of the classification errors (when errors costs are uniform, this
is equal to the total number of samples minus the number of correct classifications);</li>
<li>the total cost of the classification errors without considering samples with unkown
values;</li>
<li>a list of booleans indicating if the i-th sample was classified with an unkown value;</li>
<li>the number of samples classified with unkown values;</li>
<li>list where the i-th entry has the attribute index used for classification of the i-th
sample when an unkown value occurred;</li>
<li>list containing the time spent pruning in each fold;</li>
<li>list containing the number of nodes pruned in each fold;</li>
<li>list containing the maximum tree depth for each fold;</li>
<li>list containing the number of nodes per fold, after pruning;</li>
<li>list containing the number of valid attributes in root node in each fold;</li>
<li>Accuracy percentage obtained by classifying, in each fold, the test samples in the
most common class among training samples.</li>
</ul>
</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">tuple(list[int], int, int, float, float, list[bool], int, list[int], list[float],
list[int], list[int], list[int], list[int], list[float])</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="decision_tree.DecisionTree.get_root_node">
<code class="descname">get_root_node</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#decision_tree.DecisionTree.get_root_node" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">the TreeNode at the root of the tree. Might be None.</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">decision_tree.TreeNode or None</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="decision_tree.DecisionTree.get_trivial_accuracy">
<code class="descname">get_trivial_accuracy</code><span class="sig-paren">(</span><em>test_samples_indices</em><span class="sig-paren">)</span><a class="headerlink" href="#decision_tree.DecisionTree.get_trivial_accuracy" title="Permalink to this definition">¶</a></dt>
<dd><p>Gets the accuracy obtained by classifying every test samples in the most common class
among training samples. MUST be called after training the tree.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>test_samples_indices</strong> (<em>list[int]</em>) &#8211; indices of samples to be tested.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">the accuracy percentage.</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">float.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="decision_tree.DecisionTree.save_tree">
<code class="descname">save_tree</code><span class="sig-paren">(</span><em>filepath=None</em><span class="sig-paren">)</span><a class="headerlink" href="#decision_tree.DecisionTree.save_tree" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the tree information: nodes, attributes used to split each one, values to each side,
etc.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>filepath</strong> (<em>str or None</em>) &#8211; file in which to save the tree. If <cite>None</cite>, prints to stdout. Defaults to
<cite>None</cite>.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">None.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="decision_tree.DecisionTree.test">
<code class="descname">test</code><span class="sig-paren">(</span><em>test_sample_indices</em><span class="sig-paren">)</span><a class="headerlink" href="#decision_tree.DecisionTree.test" title="Permalink to this definition">¶</a></dt>
<dd><dl class="docutils">
<dt>Tests the (already trained) tree over samples from the same dataset as the</dt>
<dd>training set. If the tree hasn&#8217;t been trained, the program will exit.</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>test_sample_indices</strong> (<em>list[int]</em>) &#8211; list of the test set indices for samples from the same dataset
used for training.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">A tuple containing, in order:<ul class="simple">
<li>a list of predicted class for each test sample;</li>
<li>the number of correct classifications;</li>
<li>the number of correct classifications done without test samples with unkown values
(that is, values that are unkown at a TreeNode &#8211; they are classified as the most
common class at that node);</li>
<li>the total cost of the classification errors (when errors costs are uniform, this is
equal to the total number of test samples minus the number of correct
classifications);</li>
<li>the total cost of the classification errors without considering test samples with
unkown values;</li>
<li>a list of booleans indicating if the i-th test sample was classified with an unkown
value;</li>
<li>the number of test samples classified with unkown values;</li>
<li>list where the i-th entry has the attribute index used for classification of the i-th
sample when an unkown value occurred.</li>
</ul>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">tuple(list[int], int, int, float, float, list[bool], int, list[int])</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="decision_tree.DecisionTree.test_from_csv">
<code class="descname">test_from_csv</code><span class="sig-paren">(</span><em>test_dataset_csv_filepath</em>, <em>key_attrib_index</em>, <em>class_attrib_index</em>, <em>split_char</em>, <em>missing_value_string</em><span class="sig-paren">)</span><a class="headerlink" href="#decision_tree.DecisionTree.test_from_csv" title="Permalink to this definition">¶</a></dt>
<dd><p>Tests the (already trained) tree using all samples from a given csv file. If the tree
hasn&#8217;t been trained, the program will exit.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>test_dataset_csv_filepath</strong> (<em>str</em>) &#8211; path to the test dataset.</li>
<li><strong>key_attrib_index</strong> (<em>int or None</em>) &#8211; column index of the samples&#8217; keys on the csv.</li>
<li><strong>class_attrib_index</strong> (<em>int</em>) &#8211; column index of the samples&#8217; classes on the csv.</li>
<li><strong>split_char</strong> (<em>str</em>) &#8211; char used to split columns in the csv.</li>
<li><strong>missing_value_string</strong> (<em>int or None</em>) &#8211; string used to indicate that a sample does not have a value.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><p>A tuple containing, in order:</p>
<ul class="simple">
<li>a list of predicted class for each test sample;</li>
<li>the number of correct classifications;</li>
<li>the number of correct classifications done without test samples with unkown values
(that is, values that are unkown at a TreeNode &#8211; they are classified as the most
common class at that node);</li>
<li>the total cost of the classification errors (when errors costs are uniform, this is
equal to the total number of test samples minus the number of correct
classifications);</li>
<li>the total cost of the classification errors without considering test samples with
unkown values;</li>
<li>a list of booleans indicating if the i-th test sample was classified with an unkown
value;</li>
<li>the number of test samples classified with unkown values;</li>
<li>list where the i-th entry has the attribute index used for classification of the i-th
sample when an unkown value occurred.</li>
</ul>
</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">tuple(list[int], int, int, float, float, list[bool], int, list[int])</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="decision_tree.DecisionTree.train">
<code class="descname">train</code><span class="sig-paren">(</span><em>curr_dataset</em>, <em>training_samples_indices</em>, <em>max_depth</em>, <em>min_samples_per_node</em>, <em>use_stop_conditions=False</em>, <em>max_p_value_chi_sq=0.1</em><span class="sig-paren">)</span><a class="headerlink" href="#decision_tree.DecisionTree.train" title="Permalink to this definition">¶</a></dt>
<dd><p>Trains the tree in a recursive fashion, starting at the root&#8217;s TreeNode. Afterwards,
prunes the trivial subtrees.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>curr_dataset</strong> (<a class="reference internal" href="dataset.html#dataset.Dataset" title="dataset.Dataset"><em>dataset.Dataset</em></a>) &#8211; dataset containing the samples used for training.</li>
<li><strong>training_samples_indices</strong> (<em>list[int]</em>) &#8211; list containing the indices of samples of <cite>curr_dataset</cite>
used for training.</li>
<li><strong>max_depth</strong> (<em>int</em>) &#8211; maximum tree depth allowed. Zero means the root is a leaf.</li>
<li><strong>min_samples_per_node</strong> (<em>int</em>) &#8211; if a node has less than this number of training samples,
it will necessarily be a leaf.</li>
<li><strong>use_stop_conditions</strong> (<em>bool</em>) &#8211; informs wether we should use pruning techniques to avoid
using attributes with small number of samples (and, thus, avoiding statistical
anomalies). An attribute will be considered invalid if it contains less than
<cite>MIN_SAMPLES_IN_SECOND_MOST_FREQUENT_VALUE</cite> samples in the second most frequent value
(this way at least two values have this number of samples) or if a chi-square test,
applied on the attributes&#8217; contingency table has a p-value greater or equal to
<cite>max_p_value_chi_sq</cite>. When an attribute is considered invalid for the number of samples
in the second most frequent value, it will be considered invalid in every child node of
the current TreeNode. If it was considered invalid because of the chi-square test, it
can be considered valid in a descendant node. Defaults to <cite>False</cite>.</li>
<li><strong>max_p_value_chi_sq</strong> (<em>float</em>) &#8211; is the maximum p-value allowed for an attribute to be
accepted when doing chi-square tests (that is, when <cite>use_stop_conditions</cite> is <cite>True</cite>).A
p-value of 1.0 is equal to 100%. Defaults to <cite>0.1</cite> (which is 10%).</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">tuple containing the time spent pruning the trained tree and the number of nodes
pruned.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">tuple(float, int)</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="decision_tree.DecisionTree.train_and_test">
<code class="descname">train_and_test</code><span class="sig-paren">(</span><em>curr_dataset</em>, <em>training_samples_indices</em>, <em>validation_sample_indices</em>, <em>max_depth</em>, <em>min_samples_per_node</em>, <em>use_stop_conditions=False</em>, <em>max_p_value_chi_sq=0.1</em><span class="sig-paren">)</span><a class="headerlink" href="#decision_tree.DecisionTree.train_and_test" title="Permalink to this definition">¶</a></dt>
<dd><p>Trains a tree with part of the dataset (training samples) and tests the tree
classification in another part (validation samples).</p>
<p>Note that although the training and test samples are part of the same Dataset class, they
usually shouldn&#8217;t intersect.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>curr_dataset</strong> (<a class="reference internal" href="dataset.html#dataset.Dataset" title="dataset.Dataset"><em>dataset.Dataset</em></a>) &#8211; dataset containing the samples used for training.</li>
<li><strong>training_samples_indices</strong> (<em>list[int]</em>) &#8211; list containing the indices of samples of <cite>curr_dataset</cite>
used for training.</li>
<li><strong>validation_sample_indices</strong> (<em>list[int]</em>) &#8211; list containing the indices of samples of <cite>curr_dataset</cite>
used to test the tree classification.</li>
<li><strong>max_depth</strong> (<em>int</em>) &#8211; maximum tree depth allowed. Zero means the root is a leaf.</li>
<li><strong>min_samples_per_node</strong> (<em>int</em>) &#8211; if a node has less than this number of training samples,
it will necessarily be a leaf.</li>
<li><strong>use_stop_conditions</strong> (<em>bool</em>) &#8211; informs wether we should use pruning techniques to avoid
using attributes with small number of samples (and, thus, avoiding statistical
anomalies). An attribute will be considered invalid if it contains less than
<cite>MIN_SAMPLES_IN_SECOND_MOST_FREQUENT_VALUE</cite> samples in the second most frequent value
(this way at least two values have this number of samples) or if a chi-square test,
applied on the attributes&#8217; contingency table has a p-value greater or equal to
<cite>max_p_value_chi_sq</cite>. When an attribute is considered invalid for the number of samples
in the second most frequent value, it will be considered invalid in every child node of
the current TreeNode. If it was considered invalid because of the chi-square test, it
can be considered valid in a descendant node. Defaults to <cite>False</cite>.</li>
<li><strong>max_p_value_chi_sq</strong> (<em>float</em>) &#8211; is the maximum p-value allowed for an attribute to be
accepted when doing chi-square tests (that is, when <cite>use_stop_conditions</cite> is <cite>True</cite>). A
p-value of 1.0 is equal to 100%. Defaults to <cite>0.1</cite> (which is 10%).</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><p>A tuple containing the tree&#8217;s max depth in the second entry, the time taken
pruning in the third entry and the number of nodes pruned in the fourth entry. In the
first entry it returns another tuple containing, in order:</p>
<ul class="simple">
<li>a list of predicted class for each validation sample;</li>
<li>the number of correct classifications;</li>
<li>the number of correct classifications done without validation samples with unkown
values (that is, values that are unkown at a TreeNode &#8211; they are classified as the
most common class at that node);</li>
<li>the total cost of the classification errors (when errors costs are uniform, this is
equal to the total number of validation samples minus the number of correct
classifications);</li>
<li>the total cost of the classification errors without considering validation samples
with unkown values;</li>
<li>a list of booleans indicating if the i-th validation sample was classified with an
unkown value;</li>
<li>the number of validation samples classified with unkown values;</li>
<li>list where the i-th entry has the attribute index used for classification of the i-th
sample when an unkown value occurred.</li>
</ul>
</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">tuple(tuple(list[int], int, int, float, float, list[bool], int, list[int]), int,
float, int)</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="data">
<dt id="decision_tree.MIN_SAMPLES_IN_SECOND_MOST_FREQUENT_VALUE">
<code class="descclassname">decision_tree.</code><code class="descname">MIN_SAMPLES_IN_SECOND_MOST_FREQUENT_VALUE</code><em class="property"> = 40</em><a class="headerlink" href="#decision_tree.MIN_SAMPLES_IN_SECOND_MOST_FREQUENT_VALUE" title="Permalink to this definition">¶</a></dt>
<dd><p>Minimum number of samples needed in the two most frequent values of an attribute such that it is
considered valid.</p>
</dd></dl>

<dl class="data">
<dt id="decision_tree.MIN_SAMPLES_SECOND_LARGEST_CLASS">
<code class="descclassname">decision_tree.</code><code class="descname">MIN_SAMPLES_SECOND_LARGEST_CLASS</code><em class="property"> = 40</em><a class="headerlink" href="#decision_tree.MIN_SAMPLES_SECOND_LARGEST_CLASS" title="Permalink to this definition">¶</a></dt>
<dd><p>Minimum number of samples needed in the two most frequent classes such that this node can be
split.</p>
</dd></dl>

<dl class="class">
<dt id="decision_tree.NodeSplit">
<em class="property">class </em><code class="descclassname">decision_tree.</code><code class="descname">NodeSplit</code><span class="sig-paren">(</span><em>split</em>, <em>values_to_split</em>, <em>mid_point</em><span class="sig-paren">)</span><a class="headerlink" href="#decision_tree.NodeSplit" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">object</span></code></p>
<p>Data structure containing information about the best split found on a node.</p>
<p>Used for debugging and for the classification of test samples.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>split</strong> (<a class="reference internal" href="criteria.html#criteria.Split" title="criteria.Split"><em>criteria.Split</em></a>) &#8211; Split information (attribute index, split values and criterion
value)</li>
<li><strong>values_to_split</strong> (<em>dict[int, int]</em>) &#8211; reversed index for <cite>splits_values</cite>. Given a value, it returns the index
of the split that this value belongs to.</li>
<li><strong>mid_point</strong> (<em>float</em>) &#8211; cut point for numeric splits. Will be the average between the largest
value on the left split and the smallest value on the right split.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">Ival int separation_attrib_index:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><p class="first">Index of the attribute used for splitting.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name" colspan="2">Ival splits_values:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><p class="first">list containing a set of attribute values for each TreeNode child. Binary
splits have two sets (left and right split values), multiway splits may have many more.</p>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">Valtype splits_values:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><p class="first">list[set(int)]</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name" colspan="2">Ival values_to_split:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><p class="first">reversed index for <cite>splits_values</cite>. Given a value, it returns the index
of the split that this value belongs to.</p>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">Valtype values_to_split:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><p class="first">dict[int, int]</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name" colspan="2">Ival float criterion_value:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><p class="first">criterion value for this split.</p>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">Ival float mid_point:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><p class="first last">cut point for numeric splits. Will be the average between the largest
value on the left split and the smallest value on the right split.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="class">
<dt id="decision_tree.TreeNode">
<em class="property">class </em><code class="descclassname">decision_tree.</code><code class="descname">TreeNode</code><span class="sig-paren">(</span><em>curr_dataset</em>, <em>valid_samples_indices</em>, <em>valid_nominal_attribute</em>, <em>valid_numeric_attribute</em>, <em>max_depth_remaining</em>, <em>min_samples_per_node</em>, <em>use_stop_conditions=False</em>, <em>max_p_value_chi_sq=0.1</em><span class="sig-paren">)</span><a class="headerlink" href="#decision_tree.TreeNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">object</span></code></p>
<p>Contains information of a certain node of a decision tree.</p>
<p>It has information about the samples used during training at this node and also about it&#8217;s
NodeSplit.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>curr_dataset</strong> (<a class="reference internal" href="dataset.html#dataset.Dataset" title="dataset.Dataset"><em>dataset.Dataset</em></a>) &#8211; dataset of samples used for training/split generation.</li>
<li><strong>valid_samples_indices</strong> (<em>list[int]</em>) &#8211; indices of samples that should be used for training at this node.</li>
<li><strong>valid_nominal_attribute</strong> (<em>list[bool]</em>) &#8211; the i-th entry informs wether the i-th attribute is a valid
nominal one.</li>
<li><strong>valid_numeric_attribute</strong> (<em>list[bool]</em>) &#8211; the i-th entry informs wether the i-th attribute is a valid
numeric one.</li>
<li><strong>max_depth_remaining</strong> (<em>int</em>) &#8211; maximum depth that the subtree rooted at this node can have. If
zero, this node will be a leaf.</li>
<li><strong>min_samples_per_node</strong> (<em>int</em>) &#8211; minimum number of samples that must be present in order to try
to create a subtree rooted at this node. If less than this, this node will be a leaf.</li>
<li><strong>use_stop_conditions</strong> (<em>bool</em>) &#8211; informs wether we should use pruning techniques to avoid using
attributes with small number of samples (and, thus, avoiding statistical anomalies). An
attribute will be considered invalid if it contains less than
<cite>MIN_SAMPLES_IN_SECOND_MOST_FREQUENT_VALUE</cite> samples in the second most frequent value (this
way at least two values have this number of samples) or if a chi-square test, applied on the
attributes&#8217; contingency table has a p-value greater or equal to <cite>max_p_value_chi_sq</cite>. When
an attribute is considered invalid for the number of samples in the second most frequent
value, it will be considered invalid in every child node of the current TreeNode. If it was
considered invalid because of the chi-square test, it can be considered valid in a
descendant node. Defaults to <cite>False</cite>.</li>
<li><strong>max_p_value_chi_sq</strong> (<em>float</em>) &#8211; is the maximum p-value allowed for an attribute to be accepted
when doing chi-square tests (that is, when <cite>use_stop_conditions</cite> is <cite>True</cite>). A p-value of
1.0 is equal to 100%. Defaults to <cite>0.1</cite> (which is 10%).</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>is_leaf</strong> (<em>bool</em>) &#8211; True: indicates if the current TreeNode is a tree leaf.</li>
<li><strong>max_depth_remaining</strong> (<em>int</em>) &#8211; maximum depth that the subtree rooted at the current TreeNode can
still grow. If zero, the current TreeNode will be a leaf.</li>
<li><strong>node_split</strong> (<em>decision_tree.NodeSplit or None.</em>) &#8211; None: Data structure containing information about which attribute and split
values were obtained in this TreeNode with a certain criterion. Also contains the criterion
value. It is <cite>None</cite> if the current TreeNode is a leaf.</li>
<li><strong>nodes</strong> (<em>list[decision_tree.TreeNode]</em>) &#8211; []: list containing every child TreeNode from the current TreeNode.</li>
<li><strong>contingency_tables</strong> (<em>decision_tree.ContingencyTable or None.</em>) &#8211; contains a list where the i-th entry is a tuple containing two pieces
of information of the i-th attribute: the contingency table for that attribute (value index
is row, class index is column) and a list of number of times each value is attained in the
training set (i-th entry is the number of times a sample has value i in this attribute and
training dataset). Used by many criteria when calculating the optimal split. Note that, for
invalid attributes, the entry is an empty decision_tree.ContingencyTable.</li>
<li><strong>curr_dataset</strong> (<a class="reference internal" href="dataset.html#dataset.Dataset" title="dataset.Dataset"><em>dataset.Dataset</em></a>) &#8211; dataset containing the training samples.</li>
<li><strong>valid_samples_indices</strong> (<em>list[int]</em>) &#8211; contains the indices of the valid training samples.</li>
<li><strong>valid_nominal_attribute</strong> (<em>list[bool]</em>) &#8211; list where the i-th entry indicates wether the i-th attribute
from the dataset is valid and nominal or not.</li>
<li><strong>valid_numeric_attribute</strong> (<em>list[bool]</em>) &#8211; list where the i-th entry indicates wether the i-th attribute
from the dataset is valid and numeric or not.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="decision_tree.TreeNode.create_subtree">
<code class="descname">create_subtree</code><span class="sig-paren">(</span><em>criterion</em><span class="sig-paren">)</span><a class="headerlink" href="#decision_tree.TreeNode.create_subtree" title="Permalink to this definition">¶</a></dt>
<dd><p>Given the splitting criterion, creates a tree rooted at the current TreeNode.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>criterion</strong> &#8211; (Criterion) splitting criterion used to create the tree recursively.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">None</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="decision_tree.TreeNode.get_max_depth">
<code class="descname">get_max_depth</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#decision_tree.TreeNode.get_max_depth" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">the maximum depth of the tree rooted at the current TreeNode. If the current node
is a leaf, it will return zero.</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">int</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="decision_tree.TreeNode.get_most_popular_subtree">
<code class="descname">get_most_popular_subtree</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#decision_tree.TreeNode.get_most_popular_subtree" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">the number of samples in the most popular subtree. If it is leaf, returns
<cite>self.num_valid_samples</cite>.</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">int</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="decision_tree.TreeNode.get_num_nodes">
<code class="descname">get_num_nodes</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#decision_tree.TreeNode.get_num_nodes" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">the number of nodes in the tree rooted at the current TreeNode (counting includes
leaves and the current TreeNode).</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">int</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="decision_tree.TreeNode.prune_trivial_subtrees">
<code class="descname">prune_trivial_subtrees</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#decision_tree.TreeNode.prune_trivial_subtrees" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies pruning to an already trained tree. Returns the number of pruned nodes.</p>
<p>If a TreeNode is trivial, that is, every leaf in its subtree has the same
<cite>most_common_int_class</cite>, then the current TreeNode becomes a leaf with this class, deleting
every child node in this process. It is applied recursively.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">number of nodes pruned.</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">int</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="data">
<dt id="decision_tree.USE_MIN_SAMPLES_SECOND_LARGEST_CLASS">
<code class="descclassname">decision_tree.</code><code class="descname">USE_MIN_SAMPLES_SECOND_LARGEST_CLASS</code><em class="property"> = False</em><a class="headerlink" href="#decision_tree.USE_MIN_SAMPLES_SECOND_LARGEST_CLASS" title="Permalink to this definition">¶</a></dt>
<dd><p>Use the minimum number of samples in the two most frequent classes as a criteria to allow a split
or not.</p>
</dd></dl>



          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper"><div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="criteria.html" title="previous chapter">&lt;no title&gt;</a></li>
  </ul></li>
</ul>
</div>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/decision_tree.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2017, Felipe de Albuquerque Mello Pereira.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.3.6</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.7</a>
      
      |
      <a href="_sources/decision_tree.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>